---
l1idx: 3
title: "Network Load-balancing:  What are we DOING here?"   ((DRAFT))
permalink: 3.html
---


I was pulled into some discussions recently around "how/where should we do the load-balancing" for a (to me, at the time) largely opaque system architecture.  There were two camps at the time.  Camp-one wanted the system in question to send all of its outbound traffic to a network load-balancer which would distribute it to the eventual endpoints.  Camp-two wanted to system to send its outbound connections directly to their actual endpoints.

It was your typical tomato/tomahto, New-York-slice/Chicago-deep-dish situation.  The deeper I dug, the more clear it became that *both* design options were technically feasible.  It also became apparent that the strong opinions on both sides seemed to be originating from places of tunnel-vision.  "The system" was an API gateway, and "the load balancer" was an F5 LTM.  The endpoints were APIs implemented in a variety of ways, but probably most-often as services in a Kubernetes cluster.

Instead of asking ourselves "how irritating would it be to have to do this on the API gateway" and "how irritiating would it be to have to do this on the F5 LTM?", I (eventually) realized that we probbaly should have been asking ourselves:   "What does our API architecture have to say about scalability and reliability of API services?"   Why?

Because, dear reader, network load-balancers only even exist in the first place to provide a (limited) mechanism to provide horizontal scalability to network-delivered applications (that don't *have* that as a native capability).  In doing so, they also provided a mechanism for enabling additional models and parameter-values for reliability of those same networked applications.

## The Breakdown
- We had a set of API gateways running as a Kubernertes service in an OCP Cluster
- We had "a bunch" (somehwere between a handful and hundreds) of APIs on-boarding to the API gateway.
- We had those APIs running all over the place (colo-DC VMs, EC2 instances, services on Openshift K8s and EKS, etc...)
- We had a set of F5 LTMs between the external clients and the API gateways
- We couldn't decide *what* (if anything) to put between the API gateways and the actual API services

I tried to drive a fresh analysis of the festering decision in terms of scalability and reliability and came up with:
- We knew that we had a (for us) very high reliability target (99.85%) for some of the API services being onboarded
- The F5s *in front of the API gateways* weren't doing much of *anything* for the API gateways
  - Definitely nothing for horiztonal scalability; only providing a fail-over mechanism for the Kubernetes Ingress function.
- 




{% include links.html %}