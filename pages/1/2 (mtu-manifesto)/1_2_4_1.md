---
l1idx: 1
l2idx: 2
l3idx: 4
l4idx: 1
title: "Vertical Interface Design"
permalink: 1_2_4_1.html
summary: "Something's wrong with how we implement vertical interfaces for message-oriented protocols."
---

#### TL/DR

The absence of in\-stack signaling of MPU/MRU in  vertical\-interface design of message\-oriented protocols makes things really  _hard_ \.

It pushes deterministic calculations about optimal interface MTU into the hands and heads of operators who often don’t  _have_  the information they  _need_  to  _make_  those calculations

#### Verbose


##### Lack of Downstack MPU Visibility

> - Knowledge of `If:L(_n\-1_)` M_P_U is _required_ in order to infer the validity of an `If:L(_n_)` interface M_T_U value.
> 
> - We _don’t_ expose MPU to upstack\-adjacent interfaces
> 
> - As a result, `If:L(_n_)` MTU configuration requires inferences made at “Layer 8”, based on detailed knowledge of `L(_n-1_)`’s operation

##### Variable Encapsulation Overhead

> - _This_  would be a non-issue if we _had_ up-stack signaling of MPU, to enable auto-configuration of _interface _ MTU _.
> -  Instead, network operators have a _more_  complicated analysis for selecting an `L(_n_)` MTU if `L(_n_)` has variable encapsulation overhead
>   -  Is `If:L(_n-1_)`’s MPU _static_ or _variable_ ?
>     - If static, infer the _actual_ `If:L(_n-1_)` MPU (by inferring encapsulation overhead from configuration)
>     - If variable, infer the lowest _ possible_ `If:L(_n-1_)` MPU
>   - Use inferred `If:L(_n\-1_)` MPU as `If:L(_n_)` interface MTU

##### MRU Opacity

> - This isn’t really a _vertical-interface_ issue
>   - It’s a Layer-8 issue
> - We need to know `If:L(_n_)`’s M_R_U to inform our inferred optimal interface MTUs for:
>   - `If:L(n)a->z`
>   - `If:L(n-1)a->z`
>   - `If:L(n+1)a->z`

##### Current Workarounds

> - Manage interface MTU at Layer-8
>   - Do the math, to the best of your ability and knowledge
>   - When all else fails, use trial-and-error to zero in on actual effective MTU and set that as interface MTU
> - Strive for consistent end-to-end `L(_n-1_)` M_P_U across your `L(_n_)` networks
>   - If it’s always the _same_  value, you only have to do the math once
