---
l1idx: 1
l2idx: 1
l3idx: 2
l4idx: 1
title: "The Fundamental Question of Load-balancing:  'How does it scale?' "
permalink: 1_1_2_1.html
summary: "Load-balancing solves functional problems for both system reliability and capacity managment.  The ability of a given load-balancing mechanism to excel at one usually comes at the cost of compromise in the other."
---

Judged by its name, a "load balancer"'s principal function ought to be to the "balancing" of "load."  Of course, that could mean just about anything, but it's worth the time it takes to drill to down to what that's actually come to mean in practice in the network space.

Application load-balancers (ALBs), aka server load-balancers (SLB), aka Layer 3-7 load balancers have been a ubiquitous feature of network country in the land of IT services for decades and the certainly do play a key role in "balancing load."  Understanding how they do this, and why, isn't just helpful in differentiating an application load-balancer from a global load-balancer, it's also intersting.  The following explanation is drastically simplified in order to illustrate the key points:

**A Load-balancing fable

Picture an application, running on a server, and accessed by multiple clients.  The application has exclusive access to the full resources of the server (minus OS and related overhead) and is "stable enough" and "performant" enough to suit the business's needs.

Because the IT organization is delivering so well on the business's needs, the number of clients is steadily increasing.  At first the application remains performant and available, but bit by bit it starts to run slower.  Eventually, it starts to crash periodically due to the increased load.  The application is no longer performant, and it's not availability isn't what it used to be either.

The Application owners look for ways to optimize the software, and gain some ground.  The Server owners look for ways to increase capacity, and they gain some ground as well.  The IT organization is, again, triumphant, and the application remains performant and available enough that the number of clients continues to grow.  Eventually though, the application and the server run into a hard limit.  The server can only accomodate so much memory and only has so many CPU sockets.  Once all of the cruft has been removed and the logic fully optimized, the application still has to complete the fundamental logic of its functions.

The IT organization sees these hard limits coming the proverbial mile off and have to think their way around them.   They identify a few ways they could go:
   - Rewrite the application so that it can run across multiple servers.
   - Find a new server platform with much higher resource limits
   - Find an external "shim" that can allow multiple isntances of the existing application to "work together"

The list of options starts shrinking pretty quickly.  The application is commerical off-the shelf software that the IT org. simply can't rewrite.  Their hardware vendor has told them that that if they want a larger capacity server they will need to move from their current "pizza-box" form-factor to a half-rack chassis/blade enclosure system in which they would have to pay for the entire system upfront, but would be utilizing 10% of their capacity up-front and they project it would take them six years to grow into the full capacity.

The IT team realizes that the new platform option might be viable as part of a long-term, strategic effort towards platform architecture and capacity management practice, but an effort of that scale would require about two years to plan and execute appropriately.  This leaves them with the 3rd option: find an external shim that can stitch muliple instances of the application together.

The IT team's value-added reseller senses a disturbance in the force, and schedules a meeting with the IT org. to talk about the "server load-balancer" that they can provide.  This SLB sits "between" the clients and a whole "pool" of application servers, letting the IT org scale application capacity up "one server at a time", and solving their scalability problem.

The IT org. quickly realizes that not only does this "load balancer" solve their scalability problem, it also re-writes the rules of their approach to application availability.  Up to this point, they have always had two application servers.  One was used full-time, and the other was only used if the primary one failed.  They were, in effect, paying for twice the capacity that they would ever be able to use concurrently.  Their server platform came in sizes that were capable of supported roughly 100K ($45K), 50K ($20K), and 20K ($10K) application users.  Up to now, they'd been using two 100k-user size servers because they needed to have resiliency in the event a single server failed, and they were constantly operating at roughly 75K application users.

In that scenario the IT org was:
 - In need of only 75k-users of capacity
 - Paying for 200k-users of capacity with 2x 100k-user servers (at $90K)
 - Only able to access 100k-users of capacity
 - Have 100% of required capacity during single-server failure; 0% during dual-server failure


With the SLB in place, other options suddenly became available, most attractively:
  - Still need 75K-users of capacity
  - Pay for 120k-users of capacity with 6x 20k-user servers ($60K)
  - Be able to access 120k-users of capacity (160% of required capacity)
  - Have 133% of required capacity during single-server failure; 107% during dual-server failure


The introduction of the SLB solved the IT org's application scalability problem, reduced their server cost, and improved application reliability all while reducing server cost.  Of course, they *do* have to pay for the SLB, and they may have to pay per-instance licensing for the application itself.  Then again, the pair of HA SLBs they bought turns out to be usable for solving the same exact problem with ten other applications, and vendor-management is hard at work negotitating an application license based on concurrent users rather than application instances.

**Locality, state, and overhead

The coarsest of network load-balancers will maintain state at the TCP layer.  This allows it to ensure that traffic for a single TCP session will always be directed to the same back-end server.  (We can't have half a user's packets going to server 1 and the other half to server 2.)
